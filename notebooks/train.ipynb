{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdMi_p4dAf2A",
        "outputId": "d2914ce4-c6f1-4f79-fae8-2f5ea7955fde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Colab cache for faster access to the 'covid19-radiography-database' dataset.\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "tawsifurrahman_covid19_radiography_database_path = kagglehub.dataset_download('tawsifurrahman/covid19-radiography-database')\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUZGbhZXAsjF",
        "outputId": "73892265-409d-4ae1-c022-270871183df7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "40ystfSKBSSD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Generate some random chart\n",
        "x = np.linspace(0, 10, 100)\n",
        "y = np.sin(x)\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.title(\"Sample Chart\")\n",
        "\n",
        "# Save to Google Drive\n",
        "plt.savefig(\"/content/drive/MyDrive/sample_chart.png\")\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-09-07T20:30:08.762584Z",
          "iopub.status.busy": "2025-09-07T20:30:08.762063Z",
          "iopub.status.idle": "2025-09-07T20:30:08.769242Z",
          "shell.execute_reply": "2025-09-07T20:30:08.768479Z",
          "shell.execute_reply.started": "2025-09-07T20:30:08.762555Z"
        },
        "id": "qCE4nnRuAf2B",
        "outputId": "6a740078-2eba-4c9c-9027-153f9abcf0be",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "modules loaded\n"
          ]
        }
      ],
      "source": [
        "#import system libs\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import pathlib\n",
        "import itertools\n",
        "from glob import glob\n",
        "from tqdm import tqdm_notebook, tnrange\n",
        "\n",
        "# import data handling tools\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# import Deep learning Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model, load_model, save_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam, Adamax\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from tensorflow.keras.layers import Input,GlobalAveragePooling2D,Dense, Activation, BatchNormalization, Dropout, Lambda, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate , ReLU\n",
        "\n",
        "# Ignore Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print ('modules loaded')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-07T20:30:08.770624Z",
          "iopub.status.busy": "2025-09-07T20:30:08.770143Z",
          "iopub.status.idle": "2025-09-07T20:30:08.78454Z",
          "shell.execute_reply": "2025-09-07T20:30:08.783751Z",
          "shell.execute_reply.started": "2025-09-07T20:30:08.770608Z"
        },
        "id": "vCFdWAFSAf2B",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_df(data_dir):\n",
        "    image_paths = glob(os.path.join(data_dir, \"*\", \"images\", \"*.png\"))\n",
        "    mask_paths = []\n",
        "\n",
        "    for img_path in image_paths:\n",
        "        # Replace \"images\" with \"masks\" in the path\n",
        "        mask_path = img_path.replace(os.sep + \"images\" + os.sep, os.sep + \"masks\" + os.sep)\n",
        "        mask_paths.append(mask_path)\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"images_paths\": image_paths,\n",
        "        \"masks_paths\": mask_paths\n",
        "    })\n",
        "\n",
        "    return df\n",
        "\n",
        "# Function to split dataframe into train, valid, test\n",
        "def split_df(df):\n",
        "    # create train_df\n",
        "    train_df, dummy_df = train_test_split(df, train_size= 0.8)\n",
        "\n",
        "    # create valid_df and test_df\n",
        "    valid_df, test_df = train_test_split(dummy_df, train_size= 0.5)\n",
        "\n",
        "    return train_df, valid_df, test_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-07T20:30:08.786074Z",
          "iopub.status.busy": "2025-09-07T20:30:08.785828Z",
          "iopub.status.idle": "2025-09-07T20:30:08.804477Z",
          "shell.execute_reply": "2025-09-07T20:30:08.803908Z",
          "shell.execute_reply.started": "2025-09-07T20:30:08.786059Z"
        },
        "id": "bNCo3cM7Af2C",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def load_image(image_path, mask_path):\n",
        "    # read images\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_png(image, channels=3)\n",
        "    mask = tf.io.read_file(mask_path)\n",
        "    mask = tf.image.decode_png(mask, channels=3)\n",
        "    IMG_HEIGHT  =256\n",
        "    IMG_WIDTH  = 256\n",
        "    # resize\n",
        "    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])\n",
        "    mask = tf.image.resize(mask, [IMG_HEIGHT, IMG_WIDTH])\n",
        "\n",
        "    # normalize to [-1, 1] (Pix2Pix requirement)\n",
        "    image = (tf.cast(image, tf.float32) / 127.5) - 1\n",
        "    mask = (tf.cast(mask, tf.float32) / 127.5) - 1\n",
        "\n",
        "    return image, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-07T20:30:08.805296Z",
          "iopub.status.busy": "2025-09-07T20:30:08.805126Z",
          "iopub.status.idle": "2025-09-07T20:30:08.821408Z",
          "shell.execute_reply": "2025-09-07T20:30:08.820705Z",
          "shell.execute_reply.started": "2025-09-07T20:30:08.805283Z"
        },
        "id": "0OjLIwDLAf2C",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def make_dataset(df, batch_size=4, shuffle=True):\n",
        "    image_paths = df[\"images_paths\"].values\n",
        "    mask_paths = df[\"masks_paths\"].values\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))\n",
        "    dataset = dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=1000)\n",
        "\n",
        "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-07T20:30:08.822907Z",
          "iopub.status.busy": "2025-09-07T20:30:08.822687Z",
          "iopub.status.idle": "2025-09-07T20:30:08.836595Z",
          "shell.execute_reply": "2025-09-07T20:30:08.835814Z",
          "shell.execute_reply.started": "2025-09-07T20:30:08.822892Z"
        },
        "id": "JOdKzkP3Af2C",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "class InstanceNormalization(tf.keras.layers.Layer):\n",
        "    def __init__(self, epsilon=1e-5, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.gamma = self.add_weight(\n",
        "            shape=(input_shape[-1],),\n",
        "            initializer=\"ones\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.beta = self.add_weight(\n",
        "            shape=(input_shape[-1],),\n",
        "            initializer=\"zeros\",\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        mean, var = tf.nn.moments(x, axes=[1, 2], keepdims=True)\n",
        "        return self.gamma * (x - mean) / tf.sqrt(var + self.epsilon) + self.beta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-07T20:30:08.897072Z",
          "iopub.status.busy": "2025-09-07T20:30:08.896628Z",
          "iopub.status.idle": "2025-09-07T20:30:08.909647Z",
          "shell.execute_reply": "2025-09-07T20:30:08.908845Z",
          "shell.execute_reply.started": "2025-09-07T20:30:08.897053Z"
        },
        "id": "qPR74CDmAf2C",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# For generator\n",
        "def downsample(filters, size, apply_batchnorm=True):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    result = tf.keras.Sequential()\n",
        "    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "    if apply_batchnorm:\n",
        "        result.add(InstanceNormalization())\n",
        "    result.add(layers.LeakyReLU())\n",
        "    return result\n",
        "\n",
        "# For discriminator\n",
        "def downsample2(filters, size, apply_batchnorm=True):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    result = tf.keras.Sequential()\n",
        "    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "    if apply_batchnorm:\n",
        "        result.add(layers.BatchNormalization())\n",
        "    result.add(layers.LeakyReLU())\n",
        "    return result\n",
        "\n",
        "def upsample(filters, size, apply_dropout=False):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    result = tf.keras.Sequential()\n",
        "    # Step 1: upsample\n",
        "    result.add(layers.UpSampling2D(size=(2, 2), interpolation='nearest'))\n",
        "    # Step 2: conv\n",
        "    result.add(layers.Conv2D(filters, size, strides=1, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "    result.add(InstanceNormalization())\n",
        "    if apply_dropout:\n",
        "        result.add(layers.Dropout(0.5))\n",
        "    result.add(layers.ReLU())\n",
        "    return result\n",
        "\n",
        "def Generator():\n",
        "    inputs = layers.Input(shape=[256, 256, 3])\n",
        "\n",
        "    # Encoder (Downsampling)\n",
        "    down_stack = [\n",
        "        downsample(64, 4, apply_batchnorm=False),  # (128x128)\n",
        "        downsample(128, 4),                        # (64x64)\n",
        "        downsample(256, 4),                        # (32x32)\n",
        "        downsample(512, 4),                        # (16x16)\n",
        "        downsample(512, 4),                        # (8x8)\n",
        "        downsample(512, 4),                        # (4x4)\n",
        "        downsample(512, 4),                        # (2x2)\n",
        "    ]\n",
        "\n",
        "    # Decoder (Upsampling)\n",
        "    up_stack = [\n",
        "        upsample(512, 4, apply_dropout=True),  # (4x4)\n",
        "        upsample(512, 4, apply_dropout=True),  # (8x8)\n",
        "        upsample(512, 4, apply_dropout=True),  # (16x16)\n",
        "        upsample(215, 4),                      # (32x32)\n",
        "        upsample(128, 4),                      # (64x64)\n",
        "        upsample(64, 4),                      # (128x128)\n",
        "    ]\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    last = layers.Conv2DTranspose(\n",
        "        3, 4, strides=2, padding=\"same\",\n",
        "        kernel_initializer=initializer,\n",
        "        activation=\"tanh\"\n",
        "    )  # (256x256x3)\n",
        "\n",
        "    # U-Net forward pass\n",
        "    x = inputs\n",
        "    skips = []\n",
        "\n",
        "    for down in down_stack:\n",
        "        x = down(x)\n",
        "        skips.append(x)\n",
        "\n",
        "    bottleneck = skips[-1]\n",
        "    skips = reversed(skips[:-1])\n",
        "\n",
        "    x = bottleneck\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)\n",
        "        x = layers.Concatenate()([x, skip])\n",
        "\n",
        "    x = last(x)\n",
        "\n",
        "    return Model(inputs=inputs, outputs=x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-07T20:30:08.91124Z",
          "iopub.status.busy": "2025-09-07T20:30:08.911042Z",
          "iopub.status.idle": "2025-09-07T20:30:08.92637Z",
          "shell.execute_reply": "2025-09-07T20:30:08.925856Z",
          "shell.execute_reply.started": "2025-09-07T20:30:08.911218Z"
        },
        "id": "DpBpWXFXAf2D",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def Discriminator():\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    inp = layers.Input(shape=[256, 256, 3], name='input_image')\n",
        "    tar = layers.Input(shape=[256, 256, 3], name='target_image')\n",
        "\n",
        "    x = layers.concatenate([inp, tar])  # conditional input\n",
        "\n",
        "    down1 = downsample2(64, 4, False)(x)     # (128x128)\n",
        "    down2 = downsample2(128, 4)(down1)       # (64x64)\n",
        "    down3 = downsample2(256, 4)(down2)       # (32x32)\n",
        "\n",
        "    zero_pad1 = layers.ZeroPadding2D()(down3)\n",
        "    conv = layers.Conv2D(512, 4, strides=1,\n",
        "                         kernel_initializer=initializer,\n",
        "                         use_bias=False)(zero_pad1)\n",
        "    batchnorm1 = layers.BatchNormalization()(conv)\n",
        "    leaky_relu = layers.LeakyReLU()(batchnorm1)\n",
        "    zero_pad2 = layers.ZeroPadding2D()(leaky_relu)\n",
        "\n",
        "    last = layers.Conv2D(1, 4, strides=1,\n",
        "                         kernel_initializer=initializer)(zero_pad2)\n",
        "\n",
        "    return tf.keras.Model(inputs=[inp, tar], outputs=last)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-07T20:30:08.92773Z",
          "iopub.status.busy": "2025-09-07T20:30:08.927088Z",
          "iopub.status.idle": "2025-09-07T20:30:08.945355Z",
          "shell.execute_reply": "2025-09-07T20:30:08.944688Z",
          "shell.execute_reply.started": "2025-09-07T20:30:08.927705Z"
        },
        "id": "AeD1vRfRAf2D",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# -----------------------------\n",
        "# Losses\n",
        "# -----------------------------\n",
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def generator_loss(disc_generated_output, gen_output, target, LAMBDA=100):\n",
        "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "    return total_gen_loss\n",
        "\n",
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "    total_disc_loss = real_loss + generated_loss\n",
        "    return total_disc_loss\n",
        "\n",
        "# -----------------------------\n",
        "# Train step\n",
        "# -----------------------------\n",
        "@tf.function\n",
        "def train_step(input_image, target_image, generator, discriminator,\n",
        "               gen_optimizer, disc_optimizer, LAMBDA=100):\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        # generate image\n",
        "        gen_output = generator(input_image, training=True)\n",
        "\n",
        "        # discriminator outputs\n",
        "        disc_real_output = discriminator([input_image, target_image], training=True)\n",
        "        disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
        "\n",
        "        # losses\n",
        "        gen_loss = generator_loss(disc_generated_output, gen_output, target_image, LAMBDA)\n",
        "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "    # apply gradients\n",
        "    generator_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    gen_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
        "    disc_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n",
        "\n",
        "    return gen_loss, disc_loss\n",
        "\n",
        "# -----------------------------\n",
        "# Training loop with checkpointing\n",
        "# -----------------------------\n",
        "def train_pix2pix(generator, discriminator, dataset, n_epochs=50, LAMBDA=100, checkpoint_dir=None):\n",
        "    gen_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "    disc_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "\n",
        "    start_epoch = 0\n",
        "\n",
        "    # Checkpoint setup\n",
        "    if checkpoint_dir:\n",
        "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "        checkpoint = tf.train.Checkpoint(generator=generator,\n",
        "                                         discriminator=discriminator,\n",
        "                                         gen_optimizer=gen_optimizer,\n",
        "                                         disc_optimizer=disc_optimizer)\n",
        "        checkpoint_manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=5)\n",
        "\n",
        "        # Restore if checkpoint exists\n",
        "        if checkpoint_manager.latest_checkpoint:\n",
        "            checkpoint.restore(checkpoint_manager.latest_checkpoint)\n",
        "            print(f\"Restored from {checkpoint_manager.latest_checkpoint}\")\n",
        "            start_epoch = int(checkpoint_manager.latest_checkpoint.split('-')[-1])\n",
        "        else:\n",
        "            print(\"Starting from scratch\")\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(start_epoch, n_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{n_epochs}\")\n",
        "        for step, (target_image, input_image) in enumerate(dataset):\n",
        "            gen_loss, disc_loss = train_step(input_image, target_image,\n",
        "                                             generator, discriminator,\n",
        "                                             gen_optimizer, disc_optimizer, LAMBDA)\n",
        "            if step % 100 == 0:\n",
        "                print(f\"  Step {step}: Gen loss={float(gen_loss):.4f}, Disc loss={float(disc_loss):.4f}\")\n",
        "\n",
        "        # Save checkpoint at the end of the epoch\n",
        "        if checkpoint_dir:\n",
        "            checkpoint_save_path = checkpoint_manager.save(checkpoint_number=epoch+1)\n",
        "            print(f\"Checkpoint saved at {checkpoint_save_path}\")\n",
        "\n",
        "        # Visualize results\n",
        "        sample_tar, sample_inp = next(iter(dataset))\n",
        "        prediction = generator(sample_inp, training=False)\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        display_list = [sample_inp[0], sample_tar[0], prediction[0]]\n",
        "        title = [\"Input\", \"Target\", \"Generated\"]\n",
        "        for i in range(3):\n",
        "            plt.subplot(1, 3, i+1)\n",
        "            plt.title(title[i])\n",
        "            plt.imshow((display_list[i] + 1) / 2)  # scale [-1,1] -> [0,1]\n",
        "            plt.axis(\"off\")\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2025-09-07T20:30:08.94618Z",
          "iopub.status.busy": "2025-09-07T20:30:08.946009Z"
        },
        "id": "vsnPLaotAf2D",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# create DataFrame\n",
        "df = create_df(tawsifurrahman_covid19_radiography_database_path + \"/COVID-19_Radiography_Dataset/\")\n",
        "train_df, valid_df, test_df = split_df(df)\n",
        "\n",
        "# build datasets\n",
        "train_dataset = make_dataset(train_df, batch_size=4)\n",
        "valid_dataset = make_dataset(valid_df, batch_size=4)\n",
        "test_dataset = make_dataset(test_df, batch_size=4)\n",
        "\n",
        "# create models\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "Example: set checkpoint folder in Drive\n",
        "checkpoint_dir = \"/content/drive/MyDrive/pix2pix_checkpoints/\"\n",
        "\n",
        "\n",
        "train_pix2pix(generator, discriminator, train_dataset, n_epochs=50, checkpoint_dir=checkpoint_dir)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 576013,
          "sourceId": 3324348,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31090,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
